---
title: "EPILOG"
date: 2020-12-19
---

## BAIN?!

Die Dozierenden und IT-Berater Felix Lohmeier und Sebastian Meyer haben im Kurs das Handwerkszeug für die IT in Bibliotheken und Archiven praxisnah vermittelt. Wir haben diverse praktische Übungen ausgeführt und Einblick in verschiedene Bibliotheks- und Archivinformationssysteme erhalten. Während ich im Prolog die vorgegebenen Lernziele aufgelistet und meine Erwartungen formuliert habe, gilt es hier im Epilog die Lerninhalte und mein Vorgehen bei der Erstellung der Blogbeiträge zu reflektieren.

## Wurden die Lernziele erfüllt?
- **Funktionsweise spezifischer Bibliotheks- und Archivsoftware verstehen** > [Beitrag 2](https://kkbuhler.github.io/BAIN/2020/09/25/tag2.html) und [Beitrag 4](https://kkbuhler.github.io/BAIN/2020/10/09/tag4.html) 
- **richtige Software für eine spezifische Aufgabe evaluieren** > Wir haben keine Software-Produkte evaluiert - aber in fast jeder Vorlesung einen spezifischen Anwendungsbereich und die Software dazu kennengelernt :)
- **Suchmaschinen konfigurieren** > [Beitrag 9](https://kkbuhler.github.io/BAIN/2020/12/11/tag9.html)
- **Bibliothekarische und archivarische Metadaten modellieren und diese mit entsprechenden Anwendungen übertragen** > [Beitrag 8](https://kkbuhler.github.io/BAIN/2020/11/27/tag8.html)
- **Crosswalks zwischen unterschiedlichen Metadatenformaten programmieren** > Wir haben Crosswalks nicht programmiert, sondern durchgeführt > [Beitrag 7](https://kkbuhler.github.io/BAIN/2020/11/20/tag7.html)

## Wurden meine Erwartungen an den Kurs erfüllt?
Wohlweislich habe ich meinen Fokus für diesen Blog bereits von Beginn an weniger auf Ausführungen zu technischen Finessen gelegt, sondern auf die inhaltlichen Zusammenhänge. Dies hat mir erlaubt, den recht technisch ausgelegten Kurs auf einer Metaebene zu behandeln und dadurch Erkenntnisse über das grobe Ganze hinweg zu generieren. Gerade auch durch die vielleicht simpel anmutenden Fragestellungen war ich gefordert, den Sachverhalt gründlich und grundsätzlich zu erörtern. Weiter hat mich dieses Vorgehen auch krisenfrei durch den Kurs geführt, weil ich mich bei Misserfolgen bei Fehlinstallationen, der Fehlersuche in einem Code oder der Eingabe von kryptischen Befehlen nicht unter Druck gesetzt fühlte. So zeigte sich denn auch das "Suchmaschine konfigurieren, Metadaten modellieren und Crosswalks programmieren" als halb so schlimm als befürchtet. Die Bereitschaft der Dozierenden für Hilfestellungen ist verdankenswert(!), auch wenn ich am Ende nicht oft davon Gebrauch machte.

Weiter war mir als Anwenderin des Bibliotheksinformationssystem ALEPH und des Archivinformationssystem AtoM bereits ein Praxisbezug gegeben. Im Arbeitsalltag wird die Software allerdings nur sehr eingeschränkt benutzt. Ich fand es deshalb sehr interessant, über den kleinen Anwendungsbereich hinaus die Funktionsweise solcher Informationsysteme zu erfahren und verschiedene Systeme kennenzulernen. Die Hoffnung, in diesem Kurs auch [kleio](https://kleio.com) näher zu betrachten, blieb unerfüllt. Stattdessen habe ich mich in [ANTON]((https://kkbuhler.github.io/BAIN/2020/10/16/tag5.html)) vertieft. 

## Meine Erkenntnisse 
**Lerntagebuch als lehrreicher Leistungsnachweis:** Durch das Reflektieren der Lehreinheiten in Form des Lerntagebuchs wurden die Inhalte individuell vertieft. Dass der Schwerpunkt zur durchgeführten Lehrveranstaltung für den Beitrag selber definiert werden konnte, habe ich sehr geschätzt. Mich hat das motiviert, Fragen zu stellen und diese auch in manchmal ausführlichen Recherchen zu beantworten.

**Fragestellungen für Erkenntnisgewinn:** Alleine schon das Überlegen der Frage brachte mich dem Kern der Sache oft ein schönes Stück näher. Durch das Fragen offenbarten sich mir Wissenslücken, die ich durch das Beantworten der Fragen zu füllen suchte. 

**Bibliografischen Metadaten aus Sicht der IT:** Als Anwenderin von Bibliotheks- und Archivinformationssystemen im Bereich der Katalogisierung von Medien war es interessant, eine technischen Einblick in die Kulissen hinter der Systeme zu werfen. Während die Bibliotheks- oder Archivangestellte in der Regel Metadaten durch einzelne Einträge in Felder oder die gezielte Vervielfältigung von Exemplarsätzen erstellt, sind in diesem Kurs Möglichkeiten der effizienten Bearbeitung und Übernahme von grossen Datenmengen aufgezeigt worden.

**Umgang mit grossen Datenmengen erfordert erweiterte Kenntnisse:** Vor allem das Manipulieren und Importieren von grossen Datenmengen in ein Informationssystem ist ein komplexer Vorgang, der die Anwendung verschiedener Befehle erfordert. Hier scheint mir persönlich der Einbezug einer Fachperson, z.B. Systembibliothekar'in oder Software-Dienstleiter'in, durchaus sinnvoll. Es ist wohl ohnehin nicht die Meinung dieser Lehrveranstaltung, dass alle Informationswissenschaftler'innen solche Aufgaben selber lösen sollen, sondern dass sie das die Zusammenhänge verstehen und bei der Kommunikation mit den Fachpersonen eine Ahnung haben. 

**Einsatz des Terminals:** Auch wenn der Nutzen des Terminals verheissungsvoll beschrieben wurde, so konnte ich mich doch nicht richtig damit anfreunden. Zu abstrakt ist mir die Anwendung. Visuelle Oberflächen sind mir lieber, weil sie mir das klitzekleine Gefühl vermitteln, eher zu wissen, was ich gerade tu'. Zudem werde ich selber eher keine Bibliotheks- oder Archivinformationssoftware installieren - schon gar nicht via Terminal ;). In meinem Arbeitsumfeld steht diese Aufgabe spezialisiertem Personal oder dazu beauftragen Fachpersonen zu. 

**Best Practice:** Das in der letzten Vorlesung vorgestellte Best-Practice-Projekt "Deutsches Literaturarchiv Marbach" hat mich zum Kursabschluss sehr begeistert. Die in das Projekt involvierten Dozierenden haben einen Einblick in den sich im Aufbau befindenden Online-Katalog gegeben und im 1. Quartal 2021 online gehen wird. 

![]({{https://github.com/kkbuhler/}}https://raw.githubusercontent.com/kkbuhler/BAIN/master/images/literaturarchiv_ansicht treffer normdatum.PNG)

*Abb.: Neuer Online-Katalog Deutsches Literaturachiv Marbach*

Nebst der zeitgemässichen optischen Gestaltung überzeugt der Katalog auch mit der in "Typo3Find" erstellten Suchoberfläche. Die eigenen 4'000'000 Datensätze werden beispielsweise mit externer Information (Wikidata) verknüpft. Über Nacht wird die Information per OpenRefine im Cache gespeichert und dann die Datensätze damit angereichert. Dadurch wird die Datensuche im Suchindex, in Normdaten und Kombinationen davon möglich. Es wird kein standardisiertes Metadatenformat verwendet. Stattdessen kommen durch das Archivmaterial und die Bücher bestimmte, bereits im eigenen System angewandten Felder zum Zug. Das Zielformat ist CSV. Der Suchindex wurde mit SolR angelegt und mit Facetten angereichert. Der Bestand ist nach Rollen durchsuchbar. Je mehr Resourcen mit einer Normdatei verbunden sind, desto höher erscheint sie im Ranking. Es wurde uns eine Lösung vorgestellt, die während vier Jahren mit verschiedenen Beteiligten entwickelt wurde und Elemente der verschiedenen Vorlesungen vereint.

## Zum Schluss, liebe Leser'in ...
... hoffe ich, dass ich meine vielen Fragen verständlich und informativ beantwortet habe und so auch dir einige Erkenntnisse bescheren konnte!




